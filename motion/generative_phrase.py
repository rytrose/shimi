import sys
import os

sys.path.insert(1, os.path.join(sys.path[0], '..'))
from shimi import Shimi
from posenet.posenet import PoseNet
from utils.utils import Point, normalize_position, denormalize_position, denormalize_to_range, quantize, normalize_to_range
from audio.midi_analysis import MidiAnalysis
from motion.move import Move
import pygame.mixer as mixer
import random
import time
import numpy as np


class GenerativePhrase:
    """Moves Shimi according to a MIDI phrase and music/movement research."""

    def __init__(self, shimi=None, posenet=False):
        """Initializes Shimi motor controller and PoseNet skeleton detection if needed.
            shimi (Shimi, optional): Defaults to None. An instance of the Shimi motor controller class.
            posenet (bool, optional): Defaults to False. Determines whether PoseNet skeleton detection should be used.
        """
        if shimi is not None:
            self.shimi = shimi
        else:
            self.shimi = Shimi()

        self.posenet = None
        if posenet:
            self.posenet = PoseNet(
                self.shimi, on_pred=self.on_posenet_prediction)

        # PoseNet parameters
        self.face_track = False
        self.update_freq = 0.1
        self.last_update = time.time()
        self.last_pos = 0.5
        mixer.init()

    def on_posenet_prediction(self, pose, fps):
        """Called when a PoseNet prediction is made.

        Args:
            pose (dict): The pose prediction data generated by PoseNet.
            fps (float): The current FPS average of PoseNet prediction. 
        """

        # **N.B.** For simplification, this isn't being loaded from the config.yaml, where it is defined.
        #   I don't want to deal with the path nonsense at the moment, but could be a TODO
        POSENET_HEIGHT = 513
        POSENET_WIDTH = 513

        points = pose['keypoints']

        # Use nose as point of reference for face tracking
        nose = None
        for point in points:
            if point['part'] == 'nose':
                nose = Point(point['position']['x'],
                             point['position']['y'], point['score'])

        if nose:
            SCORE_THRESH = 0.7
            MOVE_THRESH = 0.05
            MIN_VEL = 40
            MAX_VEL = 100

            # Only consider PoseNet to be valid if above SCORE_THRESH (percentage) confidence
            if nose.score > SCORE_THRESH:
                if time.time() > self.last_update + self.update_freq:
                    # Calculate where to look
                    #  Camera image is flipped
                    pos = 1 - (nose.x / POSENET_WIDTH)

                    # Calculate speed based on how far to move
                    current_pos = normalize_position(self.shimi.neck_lr,
                                                     self.shimi.controller.get_present_speed([self.shimi.neck_lr])[0])
                    vel = max(MIN_VEL + abs(current_pos - pos) *
                              MAX_VEL, MIN_VEL)

                    if abs(self.last_pos - pos) > MOVE_THRESH:
                        # Only actually move the motors if specified
                        if self.face_track:
                            self.shimi.controller.set_moving_speed(
                                {self.shimi.neck_lr: vel})
                            self.shimi.controller.set_goal_position(
                                {self.shimi.neck_lr: denormalize_position(self.shimi.neck_lr, pos)})

                        self.last_pos = pos

                    self.last_update = time.time()

    def generate(self, midi_path, valence, arousal, doa_value=None, wav_path=None, both=False, mute=False,
                 random_movement=False, seed=None):
        """Compute and actuate generative gesture for a given MIDI phrase and emotion.

        Args:
            midi_path (str): Path to the MIDI file to generate gestures for.
            valence (float): Valence value in range [-1.0, 1.0].
            arousal (float): Arouse value in range [-1.0, 1.0].
            doa_value (float, optional): Defaults to None. The current measurement of input direction of arrival from Shimi's microphone array.
            wav_path (str, optional): Defaults to None. The path to a WAV file to be played when gestures are actuated.
            both (bool, optional): Defaults to False. Determines whether or not to play synthesized MIDI file with WAV file, if present.
            mute (bool, optional): Defaults to False. Determines whether to not play audio, even if file paths are given.
            random_movement (bool, optional): Defaults to False. Determines whether or not to substitute random movement over the MIDI duration.
            seed (str, optional): Defaults to None. A seed for the RNG in order to make generation system deterministic.
        """

        t = time.time()

        self.midi_analysis = MidiAnalysis(midi_path)
        tempo = self.midi_analysis.get_tempo()
        length = self.midi_analysis.get_length()

        # Create the motor moves
        moves = []

        if random_movement:
            foot = self.random_movement(self.shimi.foot, length, seed)
            moves.append(foot)
            torso = self.random_movement(
                self.shimi.torso, length, seed + int(seed / 3))
            moves.append(torso)
            neck_ud = self.random_movement(
                self.shimi.neck_ud, length, seed + int(seed / 5))
            moves.append(neck_ud)
            phone = self.random_movement(
                self.shimi.phone, length, seed + int(seed / 7))
            moves.append(phone)
            neck_lr = self.random_movement(
                self.shimi.neck_lr, length, seed + int(seed / 11))
            moves.append(neck_lr)
        else:
            foot = self.foot_movement(tempo, length, valence, arousal)
            moves.append(foot)
            torso = self.torso_movement(valence, arousal)
            moves.append(torso)
            neck_ud = self.neck_ud_movement(length, valence, arousal, torso)
            moves.append(neck_ud)
            phone = self.phone_movement_onsets(tempo, length, valence, arousal)
            moves.append(phone)

            if not self.posenet:
                if not doa_value:
                    neck_lr = self.neck_lr_movement(
                        tempo, length, valence, arousal)
                else:
                    neck_lr = self.neck_lr_doa_movement(
                        tempo, length, doa_value, valence, arousal)
                moves.append(neck_lr)

        # Load wav file if given
        if wav_path:
            mixer.music.load(wav_path)

        # Start all the moves
        for move in moves:
            move.start()

        self.face_track = True  # Turn on face tracking

        # Play audio if given
        if not mute:
            if wav_path and not both:
                mixer.music.play()
            elif wav_path and both:
                mixer.music.play()
                self.midi_analysis.play()
            else:
                # For testing, play the MIDI file back
                self.midi_analysis.play()

        # Wait for all the moves to stop
        for move in moves:
            move.join()

        self.face_track = False  # Turn off face tracking
        self.shimi.initial_position()

    def neck_lr_doa_movement(self, tempo, length, doa_value, valence, arousal):
        """Moves neck left and right according to where the microphone detects input.

        Args:
            tempo (float): Tempo of the MIDI file in seconds per beat.
            length (float): Length of the MIDI file in seconds.
            doa_value (float): The current measurement of input direction of arrival from Shimi's microphone array.
            valence (float): Valence value in range [-1.0, 1.0].
            arousal (float): Arouse value in range [-1.0, 1.0].

        Returns:
            Move: A Thread of properly sequenced movements.
        """
        # 120 left, 30 right
        normalized_doa = normalize_to_range(doa_value, 120, 30)
        normalized_arousal = (arousal + 1) / 2

        print("::: DOA: %f, normalized: %f :::" % (doa_value, normalized_doa))

        move_dur = 2 * tempo * ((1 - normalized_arousal) + 0.25)
        neck_lr_move = Move(self.shimi, self.shimi.neck_lr,
                            normalized_doa, move_dur)

        t = tempo
        delay = 0.0
        while t < length:
            rest = random.choice([True, False])
            if rest:  # Occasionally don't move, makes gesture seem more realistic
                rest_dur = 2 * tempo * random.random()
                delay += rest_dur
                t += rest_dur
            else:  # Move to approximate DOA location with emotion-dependent speed
                new_pos = normalized_doa + \
                    (random.choice([-1, 1]) * ((1 + valence) / 2) * 0.3)
                move_dur = 2 * tempo * ((1 - normalized_arousal) + 0.25)
                neck_lr_move.add_move(new_pos, move_dur, delay=delay)
                delay = 0.0
                t += move_dur

        return neck_lr_move

    def neck_lr_movement(self, tempo, length, valence, arousal):
        """Moves neck left and right according to music and movement research.

        Args:
            tempo (float): Tempo of the MIDI file in seconds per beat.
            length (float): Length of the MIDI file in seconds.
            valence (float): Valence value in range [-1.0, 1.0].
            arousal (float): Arouse value in range [-1.0, 1.0].

        Returns:
            Move: A Thread of properly sequenced movements.
        """
        # Toiviainen (2-beat rotation of upper torso)
        # Burger (High valence -> more rotation)
        # Sievers (High valence and high arousal -> smoothness)
        two_beat_dur = tempo * 2

        normalized_valence = (valence + 1) / 2
        rot_range = denormalize_to_range(normalized_valence, 0.2, 0.5)

        vel_algo = 'constant'

        if arousal >= 0:  # Continuous movement if arousal > 0
            delay = 0
            if valence >= 0:
                vel_algo = 'linear_ad'
            else:
                two_beat_dur = tempo  # Angry shakes every beat
        else:
            if arousal >= -0.5:  # Wait a beat between movements
                delay = tempo
            else:  # Wait 2 beats between movements
                delay = 2 * tempo

        # To keep deterministic for experiments, look in positive directions first
        initial_pos = 0.5 + (rot_range / 2)
        neck_lr_move = Move(self.shimi, self.shimi.neck_lr,
                            initial_pos, two_beat_dur / 2, vel_algo=vel_algo)

        t = two_beat_dur / 2
        dir = -1

        while t < length:
            new_pos = 0.5 + ((dir * rot_range) / 2)
            neck_lr_move.add_move(new_pos, two_beat_dur, delay=delay)
            t += (delay + two_beat_dur)
            dir = dir * -1

        return neck_lr_move

    def neck_ud_movement(self, length, valence, arousal, torso):
        """Moves neck up and down according to music and movement research.

        Args:
            length (float): Length of the MIDI file in seconds.
            valence (float): Valence value in range [-1.0, 1.0].
            arousal (float): Arouse value in range [-1.0, 1.0].
            torso (Move): Sequenced movements of the torso.

        Returns:
            Move: A Thread of properly sequenced movements.
        """
        # Note: ~0.2 of neck movement accounts for torso
        # looking straight: tor 0.7 neck 0.7, tor 0.8 neck 0.5, tor 0.9, neck 0.3

        # Higher valence --> more tendency to look up (correct for leaning forward)
        adjusted_valence = (valence + 1) / 2
        torso_offset = 0.2 * adjusted_valence

        # Higher arousal --> more frequent nodding, more movement
        adjusted_arousal = (arousal + 1) / 2

        # Wait between half a beat and 2 beats to nod
        half_beat = (self.midi_analysis.get_tempo() / 2)
        nod_wait = half_beat * denormalize_to_range(adjusted_arousal, 4, 1)

        # Start direction
        # direction = random.choice([-1, 1])
        direction = 1  # To keep deterministic for experiments

        # Proportion of available range (limited by torso) that can be used
        pos_range = 0
        if arousal >= 0:
            # Shorter movements for lower positive arousal
            pos_range = denormalize_to_range(arousal, 0.4, 1.0)

            # Burger (High valence -> high acceleration)
            vel_algo = 'linear_ad'
        else:
            # Short movements for less negative arousal
            pos_range = denormalize_to_range(abs(arousal), 0.4, 1.0)
            vel_algo = 'constant'

        # Keep track of timeline
        t = 0

        # Quantize nods to half beats
        while t < nod_wait:
            t += half_beat

        pos = self.calculate_neck_ud_position(
            t, torso, torso_offset, pos_range, direction)
        neck_ud_move = Move(self.shimi, self.shimi.neck_ud,
                            pos, t, vel_algo=vel_algo)
        last_move = t
        direction = not direction

        while t < length:
            if t < last_move + nod_wait:
                t += half_beat
            else:
                pos = self.calculate_neck_ud_position(
                    t, torso, torso_offset, pos_range, direction)
                neck_ud_move.add_move(pos, t - last_move)
                last_move = t
                direction = not direction

        return neck_ud_move

    def calculate_neck_ud_position(self, t, torso, torso_offset, pos_range, direction):
        """Helper to calculate neck position based on offset from torso position.

        Args:
            t (float): Time in seconds at which to calculate neck position.
            torso (Move): Sequenced movements of the torso.
            torso_offset (float): Amount neck position should be offset due to the torso.
            pos_range (float): Normalized absolute value of the range of motion for the neck.
            direction (bool): Determines the direction of movement.

        Returns:
            float: The position at which to set the neck.
        """
        # Torso offset to make it look up when bending forward
        torso_timestamps = torso.get_timestamps()
        torso_position = np.interp(t, torso_timestamps, torso.positions)
        offset = (1 - torso_position) * 10 * torso_offset

        half_range = pos_range / 2

        # Vary the distance by 20% of possible moving distance
        pos_in_range = half_range + \
            (direction * (half_range - (0.2 * random.random() * half_range)))

        return 1 - (offset + pos_in_range)

    def torso_movement(self, valence, arousal):
        """Moves torso forward and back according to music and movement research.

        Args:
            valence (float): Valence value in range [-1.0, 1.0].
            arousal (float): Arouse value in range [-1.0, 1.0].

        Returns:
            Move: A Thread of properly sequenced movements.
        """
        # Sievers (Valence --> leaning, derived from generated music contour, which inherently features this)
        contour_notes = self.midi_analysis.get_normalized_pitch_contour()

        # Higher valence --> more rapid matching to pitch contour
        smoothing_time = 0
        if valence < 0:
            valence = 0

        if valence >= 0:
            shortest_note_length = self.midi_analysis.get_shortest_note_length()
            longest_note_length = self.midi_analysis.get_longest_note_length()
            difference = longest_note_length - shortest_note_length
            smoothing_time = shortest_note_length + \
                ((1 - valence) * difference)

        # Higher arousal --> larger range of motion
        adjusted_arousal = (arousal + 1) / 2
        # Caps torso between 0.3-1.0
        torso_min = 0.7 + (0.10 * (1.0 - adjusted_arousal))
        torso_max = 0.95 + (0.05 * adjusted_arousal)

        # Keep track of timeline
        t = 0

        # Handle first note
        first_note = contour_notes.pop(0)
        initial_delay = 0

        # Find the first note to move to, per smoothing
        while first_note["start"] < smoothing_time:
            initial_delay += (first_note["end"] - t)
            t = first_note["end"]
            first_note = contour_notes.pop(0)

        torso_move = Move(self.shimi, self.shimi.torso,
                          denormalize_to_range(
                              first_note["norm_pitch"], torso_min, torso_max),
                          smoothing_time,
                          initial_delay=first_note["start"] - smoothing_time,
                          vel_algo='constant')

        t = first_note["start"]
        last_move = t

        delay = 0
        while len(contour_notes) > 0:
            note = contour_notes.pop(0)

            if note["start"] > last_move + smoothing_time:
                # Do move
                torso_move.add_move(denormalize_to_range(note["norm_pitch"], torso_min, torso_max),
                                    note["start"] - last_move,
                                    vel_algo='constant',
                                    delay=0)

                t = note["start"]
                last_move = t
                delay = 0
            else:
                delay += (note["start"] - t)
                t = note["start"]

        if len(torso_move.vel_algos) > 1:
            torso_move.vel_algos[0] = 'linear_a'
            torso_move.vel_algos[-1] = 'linear_d'

        return torso_move

    def foot_movement(self, tempo, length, valence, arousal):
        """Moves foot up and down according to music and movement research.

        Args:
            tempo (float): Tempo of the MIDI file in seconds per beat.
            length (float): Length of the MIDI file in seconds.
            valence (float): Valence value in range [-1.0, 1.0].
            arousal (float): Arouse value in range [-1.0, 1.0].

        Returns:
            Move: A Thread of properly sequenced movements.
        """

        # Calculate how often it taps its foot based on arousal
        quantized_arousals = [-1, -0.2, 0, 1]
        quantized_arousal = quantize(arousal, quantized_arousals)

        # Higher arousal --> smaller subdivision of tapping
        # Toiviainen (1-beat, 2-beat mediolateral arm movements)
        beat_periods = [4 * tempo, 2 * tempo, tempo, 0.5 * tempo]
        beat_period = beat_periods[quantized_arousals.index(quantized_arousal)]

        move_dist = 1.0
        move_dur = beat_period / 2
        move_wait = 0.0

        if valence < 0:
            # Lower valence --> shorter movement, faster
            neg_norm = 1 + valence
            # Make sure it moves at least 0.2
            move_dist = denormalize_to_range(neg_norm, 0.2, 1.0)
            # Make sure it's moving for at least 0.1s
            move_dur = denormalize_to_range(neg_norm, 0.1, 1.0) * move_dur
            move_wait = (beat_period / 2) - move_dur

        # Params for the linear accel/decel moves
        up_change_time = 0.7
        down_change_time = 0.4

        # Wait half of a beat to start, so the ictus is on foot down
        move = Move(self.shimi, self.shimi.foot, move_dist, move_dur,
                    vel_algo='linear_a',
                    vel_algo_kwarg={'change_time': up_change_time},
                    freq=0.04, initial_delay=(beat_period / 2))
        move.add_move(0.0, move_dur,
                      vel_algo='linear_d',
                      vel_algo_kwarg={'change_time': down_change_time},
                      delay=move_wait)
        t = 2 * (move_dur + move_wait)

        while t < length:
            move.add_move(move_dist, move_dur,
                          vel_algo='linear_a',
                          vel_algo_kwarg={'change_time': up_change_time},
                          delay=move_wait)
            move.add_move(0.0, move_dur,
                          vel_algo='linear_d',
                          vel_algo_kwarg={'change_time': down_change_time},
                          delay=move_wait)
            t += 2 * (move_dur + move_wait)

        return move

    def phone_movement(self, tempo, length, valence, arousal):
        """Twists the phone cradle DoF in a swaying motion according to music and movement research.

        Args:
            tempo (float): Tempo of the MIDI file in seconds per beat.
            length (float): Length of the MIDI file in seconds.
            valence (float): Valence value in range [-1.0, 1.0].
            arousal (float): Arouse value in range [-1.0, 1.0].

        Returns:
            Move: A Thread of properly sequenced movements.
        """
        # Calculate tempo of "sway" based on arousal
        quantized_arousals = [-1, -0.5, 0, 1]
        quantized_arousal = quantize(arousal, quantized_arousals)

        # Higher arousal --> faster "swaying"
        sway_periods = [4 * tempo, 2 * tempo, 2 * tempo, tempo]
        sway_period = sway_periods[quantized_arousals.index(quantized_arousal)]

        # Abs of arousal determines speed
        abs_arousal = abs(arousal)
        # Limit max speed to 50% of time
        move_dur = denormalize_to_range(
            1.0 - abs_arousal, 0.5, 1.0) * sway_period

        # If valence > 0 smooth movements with vel_algo
        if valence > 0:
            vel_algo = 'linear_ad'
        else:
            vel_algo = 'constant'

        # Distance is controlled by quadrant
        if valence >= 0 and arousal >= 0:
            sway_width = denormalize_to_range(valence + arousal, 0.1, 0.5)
        elif valence >= 0 and arousal < 0:
            sway_width = denormalize_to_range(valence + abs(arousal), 0.5, 0.1)
        elif valence < 0 and arousal >= 0:
            sway_width = denormalize_to_range(abs(valence) + arousal, 0.5, 0.1)
        else:
            sway_width = denormalize_to_range(
                abs(valence) + abs(arousal), 0.1, 0.5)

        # Direction to start is random
        # dir = random.choice([True, False])
        dir = True  # To keep deterministic for experiments

        move = Move(self.shimi, self.shimi.phone, 0.5 + (sway_width * [1, -1][int(dir)]), move_dur, vel_algo=vel_algo,
                    initial_delay=sway_period - move_dur)

        t = move_dur
        while t < (length - sway_period):
            dir = not dir
            move.add_move(0.5 + (sway_width * [1, -1][int(dir)]), move_dur, delay=sway_period - move_dur,
                          vel_algo=vel_algo)
            t += sway_period

        return move

    def phone_movement_onsets(self, tempo, length, valence, arousal):
        """Twists the phone cradle DoF based on musical onsets and according to music and movement research.

        Args:
            tempo (float): Tempo of the MIDI file in seconds per beat.
            length (float): Length of the MIDI file in seconds.
            valence (float): Valence value in range [-1.0, 1.0].
            arousal (float): Arouse value in range [-1.0, 1.0].

        Returns:
            Move: A Thread of properly sequenced movements.
        """
        contour_notes = self.midi_analysis.get_normalized_pitch_contour()
        onsets = [n["start"] for n in contour_notes]

        # first component of speed
        move_dist = denormalize_to_range((1 - abs(valence)), 0.2, 0.8)

        quantized_arousals = [-1, -0.6, 0.6, 1]
        quantized_arousal = quantize(arousal, quantized_arousals)
        dur_lengths = [2 * tempo, 1 * tempo, 0.5 * tempo, 0.25 * tempo]

        # second component of speed
        move_dur = dur_lengths[quantized_arousals.index(quantized_arousal)]

        # center the move, side_dist is the unused space on either side
        side_dist = (1 - move_dist) / 2

        # direction of movement is random, but consistent throughout the gesture
        # if random.choice([True, False]):
        if True:  # To keep deterministic for experiments
            start_pos = side_dist
            end_pos = 1 - side_dist
        else:
            start_pos = 1 - side_dist
            end_pos = side_dist

        # If valence > 0 smooth movements with vel_algo
        if valence > 0:
            vel_algo = 'linear_ad'
        else:
            vel_algo = 'constant'

        move = Move(self.shimi, self.shimi.phone,
                    start_pos, move_dur, vel_algo=vel_algo)
        t = move_dur
        while t < length:
            while onsets and onsets[0] < t:
                onsets.pop(0)
            if onsets:
                delay = onsets[0] - t
                move.add_move(end_pos, move_dur, delay=delay)
                move.add_move(start_pos, move_dur * 2)
                t += delay + (3 * move_dur)
                onsets.pop(0)
            else:
                t = length

        return move

    def random_movement(self, motor, length, seed):
        """Generates a sequence of random movements for the length of the MIDI file for one motor.

        Args:
            motor (int): The motor ID to generate random movements for.
            length (float): Length of the MIDI file in seconds.
            seed (str): A seed for the RNG to make randomness deterministic.

        Returns:
            Move: A Thread of properly sequenced movements.
        """

        random.seed(seed)

        if motor == self.shimi.torso:
            move_pos = 0.3 + (random.random() * 0.7)
        else:
            move_pos = random.random()
        move_dur = random.random() * (length / 2)

        t = move_dur

        rand_move = Move(self.shimi, motor, move_pos, move_dur)

        while t < length:
            if motor == self.shimi.torso:
                move_pos = 0.5 + (random.random() * 0.5)
            else:
                move_pos = random.random()
            move_dur = random.random() * (length / 2)
            rand_move.add_move(move_pos, move_dur)
            t += move_dur

        return rand_move
